# Smell: LLM Model Version Not Pinned

## Motivation

When using LLM models (OpenAI, Anthropic, etc.), it's important to pin specific model versions to ensure consistent behavior and reproducibility. Using unpinned models (latest, main, etc.) can lead to:

```python
#  Unpinned model version - uses latest
response = openai.ChatCompletion.create(
    model="gpt-4",  # Uses latest version
    messages=[{"role": "user", "content": "Generate text"}]
)

#  Pinned to specific model version
response = openai.ChatCompletion.create(
    model="gpt-4-0125-preview",  # Explicitly pinned version
    messages=[{"role": "user", "content": "Generate text"}]
)
```

Not pinning model versions can cause:

- Unexpected behavior changes when providers update models
- Non-reproducible results across time
- Potential breaking changes in model outputs
- Inconsistent performance characteristics


---

## Detection strategy (highâ€‘level view)

1. **AST parsing**  
   Parse the source and enrich nodes with parent pointers `(add_parent_info)`.

2. **Identify version-dependent LLM calls**  
    `isModelVersionedLLMCall(node)` verifies if explicit version is missing:

    Direct model IDs (gpt-4 vs gpt-4-0125-preview)
    Revision tags (main vs specific commit)
    Latest tags in model references

3. **Check model version pinning**
    `hasNoModelVersionPinning(node)` verifies if explicit version is missing:

    Direct model IDs (gpt-4 vs gpt-4-0125-preview)
    Revision tags (main vs specific commit)
    Latest tags in model references

4. **Reporting**  

   REPORT: LLM call without model version pinning at line`n `.

---


## Examples 

```python

# Unpinned versions

# OpenAI - using latest version
completion = openai.ChatCompletion.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Text"}]
)

# Anthropic - using main version 
response = anthropic.messages.create(
    model="claude-3",
    messages=[{"role": "user", "content": "Text"}]
)

# HuggingFace - no revision specified
model = AutoModelForCausalLM.from_pretrained("bigscience/bloom")

# Ollama - no tag
subprocess.run(["ollama", "run", "llama2"])
```

```python

# Pinned versions

# OpenAI - explicit version
completion = openai.ChatCompletion.create(
    model="gpt-4-0125-preview",
    messages=[{"role": "user", "content": "Text"}]
)

# Anthropic - explicit version
response = anthropic.messages.create(
    model="claude-3-20240229",
    messages=[{"role": "user", "content": "Text"}]
)

# HuggingFace - explicit revision
model = AutoModelForCausalLM.from_pretrained(
    "bigscience/bloom",
    revision="v2.1.0"
)

# Ollama - explicit tag
subprocess.run(["ollama", "run", "llama2:7b-v2"])
```

## Limitations 

- May not detect version pinning through variables or custom wrappers
- Cannot verify validity of pinned versions
- Does not track indirect model loading
- May miss custom model registries/hubs