# Smell: LLM System Message Not Set

## Motivation

When using chat-based LLM APIs (OpenAI Chat, Anthropic, etc.), it's important to set a system message that defines the assistant's behavior and constraints. Missing system messages can lead to:

```python
# ❌ No system message to guide the model
response = openai.ChatCompletion.create(
    model="gpt-4",
    messages=[
        {"role": "user", "content": "Generate a story"}
    ]
)

# ✅ System message defines behavior
response = openai.ChatCompletion.create(
    model="gpt-4",
    messages=[
        {"role": "system", "content": "You are a creative storyteller. Keep stories family-friendly."},
        {"role": "user", "content": "Generate a story"}
    ]
)
```

Not setting a system message can cause:

- Inconsistent model behavior across conversations
- Lack of control over output style and constraints
- Missing context about intended use case
- Potential unsafe/unwanted outputs


---

## Detection strategy (high‑level view)

1. **AST parsing**  
   Parse the source and enrich nodes with parent pointers `(add_parent_info)`.

2. **Identify chat-based LLM calls**  
    `isRoleBasedLLMChat(node)` detects:

    OpenAI Chat Completions
    Anthropic Messages
    OpenAI Responses
    Gemini/Vertex AI chat methods

3. **Check system message**
    `hasNoSystemMessage(node)` verifies if a system message is missing:

    In messages array
    Via system parameter
    In instructions field
    Through system_instruction kwarg

4. **Reporting**  

   REPORT: LLM chat call without a system message at line `n `.

---


## Examples 

```python

# Missing system message

# OpenAI Chat - no system role
chat = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Hello"}]
)

# Anthropic - no system param
response = anthropic.messages.create(
    model="claude-3",
    messages=[{"role": "user", "content": "Hi"}]
) 

# Gemini - empty system_instruction
model = GenerativeModel("gemini-pro")
chat = model.start_chat(
    system_instruction=""
)

```python

# System message present

# OpenAI Chat - system role set
chat = client.chat.completions.create(
    model="gpt-4",
    messages=[
        {"role": "system", "content": "You are a helpful assistant"},
        {"role": "user", "content": "Hello"}
    ]
)

# Anthropic - system param
response = anthropic.messages.create(
    model="claude-3",
    messages=[{"role": "user", "content": "Hi"}],
    system="You are an AI assistant focused on accuracy"
)

# Gemini - system instruction
model = GenerativeModel("gemini-pro")
chat = model.start_chat(
    system_instruction="Be direct and concise in your responses"
)
```

## Limitations 

- Cannot detect system messages passed through variables
- May miss custom wrappers around chat APIs
- Conservative approach - doesn't flag if system presence is uncertain
- Doesn't analyze quality/content of system messages