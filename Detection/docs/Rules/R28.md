# Smell: LLM Calls Without Bounded Metrics

## Motivation


When using LLM APIs (OpenAI, Anthropic, etc.), it's important to explicitly limit consumed resources through metrics like token count, timeout, or response size. Not specifying these bounds can lead to:

```python
#  No resource limits
response = openai.ChatCompletion.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Generate text"}]
)

#  Explicitly bounded resources
response = openai.ChatCompletion.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Generate text"}],
    max_tokens=500,  # Limit output size
    timeout=30       # Limit response time
)
```

Not setting these metrics can cause:

- Unexpected/excessive costs
- Unpredictable response times
- Uncontrolled resource usage
- Performance/stability issues


---

## Detection strategy (highâ€‘level view)

1. **AST parsing**  
   Parse the source and enrich nodes with parent pointers `(add_parent_info)`.

2. **Identify LLM calls**  
    `isLLMCall(node)` detects:

    Direct API calls (OpenAI/Anthropic/etc.)
    Client/wrapper calls
    Text generation pipelines

3. **Check metrics**
    `hasNoBoundedMetrics(node)` erifies absence of:

    max_tokens/max_output_tokens
    timeouts
    response size limits
    generation configs
4. **Exclusions**
    `isNotSDKClient(node)` excludes:

    OpenAI SDK client
    Client configuration options

5. **Reporting**  

   REPORT: LLM call without bounded metrics at line `n `.

---


## Examples 

```python

# Unbounded metrics

# OpenAI - no token/timeout limits
chat = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "..."}]
)

# Anthropic - no resource constraints
response = anthropic.messages.create(
    model="claude-3",
    messages=[{"role": "user", "content": "..."}]
)

# Gemini - no max_output_tokens
model = GenerativeModel("gemini-pro")
response = model.generate_content("...")
```

```python

# Bounded metrics

# OpenAI - explicit limits
chat = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "..."}],
    max_tokens=1000,
    timeout=60
)

# Anthropic - with timeout
with client.with_options(timeout=30) as c:
    response = c.messages.create(
        model="claude-3",
        max_tokens=2000,
        messages=[{"role": "user", "content": "..."}]
    )

# Gemini - generation config
config = {
    "max_output_tokens": 1000,
    "temperature": 0.7
}
model = GenerativeModel("gemini-pro")
response = model.generate_content("...", generation_config=config)
```

## Limitations 

- Cannot detect bounds through variables
- Conservative on dynamic parameters
- Does not check limit values
- May miss some implicit limits