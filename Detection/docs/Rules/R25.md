# Smell: LLM Temperature Not Explicitly Set

## Motivation

When using language models (OpenAI, Anthropic, etc.), the temperature parameter controls the degree of randomness/creativity in generated responses. An unspecified temperature uses the default value which may not be suitable for the use case:

```python
# Temperature not specified - uses default value
response = openai.ChatCompletion.create(
    model="gpt-3.5-turbo",
    messages=[{"role": "user", "content": "Generate a story"}]
)

# Temperature explicitly set according to needs
response = openai.ChatCompletion.create(
    model="gpt-3.5-turbo",
    messages=[{"role": "user", "content": "Generate a story"}],
    temperature=0.7  # More creative
)
```

Not explicitly setting the temperature can lead to:

- Responses that are too deterministic or too random depending on default value
- Lack of control over output creativity/variability
- Inconsistent behavior across different LLM providers


---

## Detection strategy (highâ€‘level view)

1. **AST parsing**  
   Parse the source and enrich nodes with parent pointers `(add_parent_info)`.

2. **Identify LLM calls**  
    `iisLLMCall(node)` detects different types of LLM calls:
    Direct API calls (OpenAI, Anthropic, etc.)
    LangChain constructors (ChatOpenAI, etc.)
    Text generation pipelines
    Gemini/Vertex AI calls

3. **Check temperature parameter**
    `hasNoTemperatureParameter(node)` verifies if temperature is absent:
    In direct named arguments
    Via kwargs (**params)
    In parameter dictionaries

4. **Reporting**  

   REPORT: LLM call without explicit temperature parameter at line`n `.

---


## Examples 

```python
# Unspecified temperature
completion = openai.Completion.create(
    model="text-davinci-003",
    prompt="Generate a story"
)

chat = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Explain a concept"}]
)

# Explicit temperature
completion = openai.Completion.create(
    model="text-davinci-003",
    prompt="Generate a story",
    temperature=0.2  # More deterministic/focused responses
)

chat = client.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Explain a concept"}],
    temperature=0.0  # Most deterministic mode possible
)
```

## Limitations 

- Does not detect temperature parameters defined via variables with custom names
- Does not verify temperature values (only presence/absence)
- Possible false positives on custom clients named similarly
- Does not track indirect calls (via wrapper functions)